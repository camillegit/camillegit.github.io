---
title: "Le langage, de Shannon à Wittgenstein"
author: camille              # for single entry
date: 2025-04-08 12:00:00 +0100
categories: [notes de lecture]
tags: [langage,nlp,maths]
math: true   
---

Avec l'explosion récente de la popularité de l'intelligence artificielle et en particulier des modèles de langages, j'ai ressenti le besoin de me replonger dans l'oeuvre majeure de Claude Shannon, la *théorie mathématique de la communication*. L'auteur ne sera pas inconnu à tout ingénieur qui se sera frotté un jour au traitement de signal, notamment pour son théorème de l'échantillonage. J'aimerais explorer et partager ici les liens entre l'ouvrage précurseur de Shannon et les fondement du langage dans lequel nous nous exprimons en tant que concept mathématique. Cela permet de comprendre l'évolution des modèles de génération de langage automatique mais aussi de donner quelques perspectives philosophiques sur notre principal moyen d'expression et le sens profond de la communication.

## 1. Le langage, un message comme un autre ?

L'ouvrage de Claude Shannon compile pas loin de 10 ans de travail. L'enjeu est de formaliser la transmission d'un signal. Ce travail fait avancer un nombre considérable de sujets d'importance cruciale dans les communications numériques modernes comme la compression, la robustesse au bruit et les limites de leur performance. Pour Shannon, un message est une combinaison de symboles sélectionné dans un ensemble de combinaisons de symboles possibles. Cet ensemble est l'ensemble de tout ce qui est exprimable. Cet ensemble constitue ce que l’on peut appeler un espace de communication, au sens d'espace mathématique vectoriel. Chaque message correspond donc à une suite de symboles (par exemple des bits 0 et 1 dans notre ordinateur) qui sera codée, transmise via un canal, puis décodée à l’autre extrémité. De ce point de vue, le langage humain peut lui aussi être vu comme une suite de symboles choisis dans un espace fini, constitué de mots parmi le dictionnaire, eux-mêmes constitués de syllabes, elles-même constituées de lettres de l'alphabet latin.  Une phrase que nous prononçons est ainsi l’équivalent d’un message binaire : une sélection unique et délibérée parmi une multitude de combinaisons possibles. 

Cependant le message n'existe pas de façon immanente, pour lui-même. Il intervient pour transmettre une information, une sensation, un concept entre les interlocuteurs. Ce que le message s'efforce de transmettre appartient à l'espace sémantique, c'est sa signification. Cette significaiton se réfère ou est corrélée selon certaines modalités à des entités physiques ou conceptuelles. Imaginons ici que le langage sert tout à la fois à nous conter l'épopée d'Enée ou transmettre la connaissance de la physique quantique.

Le modèle de communication de Shannon s'énonce assez simplement comme suit : 

Source → Emetteur → Canal → Récepteur → Destinataire

La source et le destinataire, dans le contexte du langage, souhaitent échanger un élément appartenant à l'espace sémantique, mettons une sensation comme la mélancolie. Ainsi Enée, la source, pourrait encoder cet élément sémantique dans le langage de la façon suivante "Sunt lacrimae rerum", peu de mots pour porter l'immense tragédie de la chute de Troie. Le destinataire, pour peu que celui-ci soit latiniste transcrira les mots dans son propre espace sémantique, espace très certainement différent d'Enée compte tenu des différences d'expériences vécues, de culture. Dès lors, il relève presque du miracle que le concept sémantique transmis arrive inchangé au destinataire compte tenu de toutes les transformations nécessaires au passage d'un espace à l'autre, comme autant de diffraction. De plus il est intéressant de noter que compte tenu de l'encodage choisi, c'est à dire des mots retenus, le message est intrinsèquement ambigu. Cette ambiguité apparaît magnifiquement en français, où les traductions acceptées pourraient être "les choses sont des larmes" ou bien "il y a des larmes dans les choses". Loin de s'exclurent, ces interprétations se superposent comme des états quantiques et enrichissent encore la portée sémantique du message.


## 2. Economie de signal et probabilisme

Créer un message, nous l'avons vu, consiste à choisir une séquence dans un ensemble fini de symboles pour créer une combinaison ordonnée unique. Il est très amusant de voir comment l'espace de communication qui est constitué de l'ensemble des séquences possibles évolue pour d'adapter aux contraintes du canal de communication. Souvenons du temps pas si lointain où nous échangions des SMS contraints en nombre de caractères (ou pour les lecteurs d'âge canonique, les télégrammes facturés au caractère transmis). Nous avions alors tout intérêt à réduire le nombre de caractères employés pour nous exprimer, ainsi sont nés certaines combinaisons de symboles tels que "bjr", "slt", "a+" ou encore "hb". Dans un même esprit d'économie de temps, le télégraphe employait les signaux les plus courts pour les symboles les plus fréquents, réduisant le temps moyen de transmission (en morse un simple court pour le "e", lettre la plus fréquente en anglais comme en français).

Ainsi on voit que la nécessité de frugalité dans la construction d'un code pour exprimer le message fait intervenir une dimension probabiliste. Considérons une approximation aléatoire d'ordre 0 parmi les symboles de l'alphabet latin, ainsi les symboles utilisés sont équiprobables et indépendants, nous obtenons par exemple "QUAKVCEOAPJFEZB". Ensuite, une approximation d'ordre 1 utilisera une suite de symboles indépendants mais cette fois avec une probabilité tirée d'un corpus anglais "OCREO HLI REGWR", encore difficile d'y comprendre quoi que ce soit d'intelligible. Passons directement à une approximation du troisième ordre (un trigram, qui se généralise en n-gram) où l'on tient compte des deux lettre précédentes selon les probabilités d'occurence dans la langue anglaise, nous obtenons "IN NO IST LAT WHEY", nous pouvons presque deviner qu'il s'agit d'un idiome anglo-saxon. En généralisant nous pouvons considérer les mots comme symbole élémentaire, ainsi une approximation du second ordre donnera par exemple "THE HEAD AND IN FRONTAL ATTACK", un charabia certes mais qui nous paraît de plus en plus plausible. Cela nous met sans équivoque sur la voie des techniques de génération automatique du langage (faisant partie du champ du Natural Language Processing). Shannon d'ailleurs lui-même établissait "un processus stochastique suffisamment complexe donnera une représentation staisfaisante d'une source discrète", en 1948 !

On admet alors que les probabilités nous ammènent à former un langage artificiel qui peut nous sembler pertinent, cependant quelle sémantique cela peut-il véhiculer et comment la signification et la probabilité s'articulent dans l'évolution d'une langue ?

## 3. L'entropie selon Shannon comme mesure de l'information

Que nous apprend l'étude des probabilités d'un code ou d'une langue sur l'information qu'il est capable de véhiculer ? Shannon répond à cette question en introduisant le concept d'entropie (inspiré de la mécanique statistique) : 

$$
H = -K \sum_{i=1}^n p_i \log p_i
$$

où $$H$$ définit l'entropie en bits d'une source puisant dans un nombre de symboles $$n$$, $$K$$ est une constante, et enfin $$p_i$$ la probabilité associée à chaque symbole. 

L'entropie représente l'incertitude d'une source mais également la quantité d'information qu'elle peut produire pour un certain nombre de symboles. Pour comprendre sa portée, étudions quelques cas extrêmes. Dans le cas où le code est composée de symboles équiprobables, $$H$$ augmente, c'est à dire que l'incertitude sur les symboles choisis augmente et ainsi l'information contenu dans chaque nouveau symbole lorsqu'il est choisi par la source. A l'opposé, lorsque la probabilté d'un symbole atteint 100% au détriment de tous les autres dont la probabilité devient évidemment 0%, on comprend que l'incertitude est très faible car le prochain symbole choisi est entièrement déterminé, l'information contenue est donc pauvre car déjà connue ou largement anticipable dans un cas moins extrême.

L'entropie peut également être conditionnelle et ainsi prendre en compte des probabilités conditionnelles mettant en exergue l'information en tant que dépendance entre les différents symboles d'une séquence, qui semble intuitivement pertinente comme montrée par l'exemple de n-grams. 

Le complément de l'entropie est appelée redondance. En effet, ce qui ne relève pas de l'incertitude est en quelque mesure prévisible et ainsi n'apporte pas d'information, vient seulement répéter une information connue. Loin d'être inutile cette redondance dans le signal permet de rendre le code robuste au bruit. Si l'on venait à éliminer arbitrairement un mot ... cette phrase, le sens en serait toujours intelligible car ce "de" vient de façon assez certaine dans le contexte de la phrase, il était très probable et donc d'une entropie assez faible. Deux cas extrême de redondance dans la langue anglaise sont souvent utilisés pour illustrer cela, le Basic English et le "Finnegans Wake" de James Joyce. Le Basic English comporte 850 mots de vocabulaire et sa redondance est très élevée. Il est nécessaire d'utiliser un grand nombre de symboles pour exprimer un concept sémantique. En revanche, James Joyce utilise un vocabulaire riche, possédant donc une entropie élevée, accroissant la densité d'information. James Joyce réalise une compression du contenu sémantique. Il en va de même des codes adaptés à une réalité particulère comme le langage marin, celui-ci permet également une compression du contenu sémantique notable en vue d'accroître l'efficacité de la communication. 

## 4. La sémantique et le monde